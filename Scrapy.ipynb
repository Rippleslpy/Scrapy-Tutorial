{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "install scrapy from Anaconda envrionment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Minimal working example\n",
    "scrapy is used to Scrape Web Pagesrite. In this example, we will build a crawler to scrape and parse the hat selling information on Amazon and store the data to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) create a project\n",
    "scrapy uses terminal (Mac) or Command Prompt (Windows) to create project.\n",
    "The project is contained in a folder, including project configuration information (.cfg), py documents (items.py, pipelines.py, settings.py) specify the contain of the project. We can rewrite the documents\n",
    "to do the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'working_example', using template directory '/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/apple/Desktop/scrapy_project/working_example\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd working_example\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "# we will create a project named working_example\n",
    "!scrapy startproject working_example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy.ipynb    \u001b[34mworking_example\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/scrapy_project/working_example\n"
     ]
    }
   ],
   "source": [
    "%cd working_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapy.cfg      \u001b[34mworking_example\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\r\n",
      "#\r\n",
      "# For more information about the [deploy] section see:\r\n",
      "# https://scrapyd.readthedocs.org/en/latest/deploy.html\r\n",
      "\r\n",
      "[settings]\r\n",
      "default = working_example.settings\r\n",
      "\r\n",
      "[deploy]\r\n",
      "#url = http://localhost:6800/\r\n",
      "project = working_example\r\n"
     ]
    }
   ],
   "source": [
    "!cat scrapy.cfg\n",
    "# scrapy.cfg shows the configuration information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 1.1.1 - project: working_example\n",
      "\n",
      "Usage:\n",
      "  scrapy <command> [options] [args]\n",
      "\n",
      "Available commands:\n",
      "  bench         Run quick benchmark test\n",
      "  check         Check spider contracts\n",
      "  commands      \n",
      "  crawl         Run a spider\n",
      "  edit          Edit spider\n",
      "  fetch         Fetch a URL using the Scrapy downloader\n",
      "  genspider     Generate new spider using pre-defined templates\n",
      "  list          List available spiders\n",
      "  parse         Parse URL (using its spider) and print the results\n",
      "  runspider     Run a self-contained spider (without creating a project)\n",
      "  settings      Get settings values\n",
      "  shell         Interactive scraping console\n",
      "  startproject  Create new project\n",
      "  version       Print Scrapy version\n",
      "  view          Open URL in browser, as seen by Scrapy\n",
      "\n",
      "Use \"scrapy <command> -h\" to see more info about a command\n"
     ]
    }
   ],
   "source": [
    "# show Available tool commands\n",
    "!scrapy -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) spider\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from websites. \n",
    "Spider uses scrapy.spiders to define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "Follwing are some attributes you can specify by spider:\n",
    "* name: identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "* start_requests(   ): must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "* parse(  ): a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it. The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) create spider using genspider\n",
    "genspider is used to create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The <name> parameter is set as the spider’s name, while <domain> is used to generate the allowed_domains and start_urls spider’s attributes.<br>\n",
    "-Syntax: scrapy genspider [-t template] <name> <domain><br>\n",
    "-genspider doesn't require project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'firstspider' using template 'basic' in module:\r\n",
      "  working_example.spiders.firstspider\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider firstspider firstspider.com\n",
    "#you can use scrapy toolst to create a new spider\n",
    "#Note:some Scrapy commands (like crawl) must be run from inside a Scrapy project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available templates:\r\n",
      "  basic\r\n",
      "  crawl\r\n",
      "  csvfeed\r\n",
      "  xmlfeed\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider -l # show available templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'example' using template 'basic' in module:\r\n",
      "  working_example.spiders.example\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider example example.com\n",
    "# create spider 'example' using template 'basic' in module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'scrapyorg' using template 'crawl' in module:\r\n",
      "  working_example.spiders.scrapyorg\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider -t crawl scrapyorg scrapy.org\n",
    "# created spider 'scrapyorg' using template 'crawl' in module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list\n",
    "List all available spiders in the current project. The output is one spider per line.\n",
    "Syntax: scrapy list\n",
    "Requires project: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\r\n",
      "firstspider\r\n",
      "scrapyorg\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check\n",
    "Syntax: scrapy check [-l] <spider><br>\n",
    "check requires project to be existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!scrapy check -l \n",
    "#checks current spiders and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 0 contracts in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy check\n",
    "#check status of current spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) create spider using scrapy.Spider\n",
    "you can also use scrapy.spider to create a spider.<br>\n",
    "Usually, a created spider is stored in spiders directory and executed by crawl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/scrapy_project/working_example/working_example/spiders\n"
     ]
    }
   ],
   "source": [
    "%cd working_example/spiders\n",
    "# open spider directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py    \u001b[34m__pycache__\u001b[m\u001b[m    example.py     firstspider.py scrapyorg.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# shows spiders that already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!touch Basic_spider.py\n",
    "# create a new spider named Basic_spider\n",
    "# write information about how to scrape a website into this spider document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will create a spider named Basic_spider that extracts information from simon business school's faculty website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Basic_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Basic_spider.py\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'faulty'\n",
    "    allowed_domains = ['simon.rochester.edu']\n",
    "    start_urls = ['http://www.simon.rochester.edu/faculty-and-research/faculty-directory/index.aspx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch Advanced_spider.py\n",
    "# create another spider named Advanced_spider\n",
    "# put more detailed information about how to scrape a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced_spider extracts only the names and links related to names of simon business school's faulty website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Advanced_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Advanced_spider.py\n",
    "import scrapy\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'faulty'\n",
    "    allowed_domains = ['simon.rochester.edu']\n",
    "    start_urls = ['http://www.simon.rochester.edu/faculty-and-research/faculty-directory/index.aspx'\n",
    "                 ]\n",
    "    def parse(self, response):\n",
    "        hxs = HtmlXPathSelector(response)\n",
    "        titles = hxs.xpath(\"//td[@class='name']\")\n",
    "        for title in titles:\n",
    "            name = title.select(\"a/text()\").extract()\n",
    "            link = title.select(\"a/@href\").extract()\n",
    "            print (title, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced_spider.py __init__.py        example.py         scrapyorg.py\r\n",
      "Basic_spider.py    \u001b[34m__pycache__\u001b[m\u001b[m        firstspider.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# now we have these two new spiders in our folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) items\n",
    "items.py document contains a item class, named by your project. It defines the fields you want to scrape from the web. You can rewrite the items document using %%writefile in iPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/scrapy_project/working_example/working_example\n",
      "__init__.py  \u001b[34m__pycache__\u001b[m\u001b[m  items.py     pipelines.py settings.py  \u001b[34mspiders\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls\n",
    "# returns back to root directory, where we can find the items.py document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "# Define here the models for your scraped items\r\n",
      "#\r\n",
      "# See documentation in:\r\n",
      "# http://doc.scrapy.org/en/latest/topics/items.html\r\n",
      "\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class WorkingExampleItem(scrapy.Item):\r\n",
      "    # define the fields for your item here like:\r\n",
      "    # name = scrapy.Field()\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can rewrite the item class. Let it contain contain the item name and link to open the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile items.py\n",
    "\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "class WorkingExampleItem(Item):\n",
    "    title = Field() # title is the item name\n",
    "    link = Field() # link is the like to open the item selling information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run items.py\n",
    "# run items.py to let the item information containing in jupyter notebook\n",
    "# items.py can also be used to specify newly created spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from scrapy.item import Item, Field\r\n",
      "\r\n",
      "class WorkingExampleItem(Item):\r\n",
      "    title = Field() # title is the item name\r\n",
      "    link = Field() # link is the like to open the item selling information"
     ]
    }
   ],
   "source": [
    "!cat items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) crawl\n",
    "Syntax: scrapy crawl <spider>\n",
    "Requires project: yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-19 17:11:14 [scrapy] INFO: Scrapy 1.1.1 started (bot: working_example)\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Overridden settings: {'BOT_NAME': 'working_example', 'NEWSPIDER_MODULE': 'working_example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['working_example.spiders']}\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Spider opened\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-02-19 17:11:14 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-02-19 17:11:14 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/robots.txt>: 'float' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n",
      "    result = f(*args, **kw)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\n",
      "    return handler.download_request(request, spider)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\n",
      "    return agent.download_request(request)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\n",
      "    parsedURI.originForm)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\n",
      "    d = self._pool.getConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\n",
      "    return self._newConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\n",
      "    return endpoint.connect(factory)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\n",
      "    timeoutDelay = sum(timeout)\n",
      "TypeError: 'float' object is not iterable\n",
      "2017-02-19 17:11:14 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/faculty-and-research/faculty-directory/index.aspx>\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n",
      "    result = f(*args, **kw)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\n",
      "    return handler.download_request(request, spider)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\n",
      "    return agent.download_request(request)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\n",
      "    parsedURI.originForm)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\n",
      "    d = self._pool.getConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\n",
      "    return self._newConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\n",
      "    return endpoint.connect(factory)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\n",
      "    timeoutDelay = sum(timeout)\n",
      "TypeError: 'float' object is not iterable\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Closing spider (finished)\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 2,\n",
      " 'downloader/exception_type_count/builtins.TypeError': 2,\n",
      " 'downloader/request_bytes': 501,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 2, 19, 22, 11, 14, 824002),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 2, 19, 22, 11, 14, 395728)}\n",
      "2017-02-19 17:11:14 [scrapy] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# we can crawl the website by name defined by our spider\n",
    "!scrapy crawl faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/scrapy_project/working_example/working_example/spiders\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Scrapy 1.1.1 started (bot: working_example)\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Overridden settings: {'BOT_NAME': 'working_example', 'NEWSPIDER_MODULE': 'working_example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['working_example.spiders']}\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Spider opened\n",
      "2017-02-19 17:11:16 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-02-19 17:11:16 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-02-19 17:11:17 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/robots.txt>: 'float' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n",
      "    result = f(*args, **kw)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\n",
      "    return handler.download_request(request, spider)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\n",
      "    return agent.download_request(request)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\n",
      "    parsedURI.originForm)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\n",
      "    d = self._pool.getConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\n",
      "    return self._newConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\n",
      "    return endpoint.connect(factory)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\n",
      "    timeoutDelay = sum(timeout)\n",
      "TypeError: 'float' object is not iterable\n",
      "2017-02-19 17:11:17 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/faculty-and-research/faculty-directory/index.aspx>\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n",
      "    result = result.throwExceptionIntoGenerator(g)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n",
      "    result = f(*args, **kw)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\n",
      "    return handler.download_request(request, spider)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\n",
      "    return agent.download_request(request)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\n",
      "    parsedURI.originForm)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\n",
      "    d = self._pool.getConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\n",
      "    return self._newConnection(key, endpoint)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\n",
      "    return endpoint.connect(factory)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\n",
      "    timeoutDelay = sum(timeout)\n",
      "TypeError: 'float' object is not iterable\n",
      "2017-02-19 17:11:17 [scrapy] INFO: Closing spider (finished)\n",
      "2017-02-19 17:11:17 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 2,\n",
      " 'downloader/exception_type_count/builtins.TypeError': 2,\n",
      " 'downloader/request_bytes': 501,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 2, 19, 22, 11, 17, 369100),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 2, 19, 22, 11, 16, 943446)}\n",
      "2017-02-19 17:11:17 [scrapy] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# or we can use runspider command to scrape a website\n",
    "%cd spiders\n",
    "!scrapy runspider Basic_spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# export the output to csv\n",
    "output = !scrapy crawl faulty\n",
    "import numpy as np\n",
    "np.savetxt('faulty.csv',output, delimiter = \",\", fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-19 17:11:24 [scrapy] INFO: Scrapy 1.1.1 started (bot: working_example)\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Overridden settings: {'BOT_NAME': 'working_example', 'NEWSPIDER_MODULE': 'working_example.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['working_example.spiders']}\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Enabled extensions:\r\n",
      "['scrapy.extensions.corestats.CoreStats',\r\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\r\n",
      " 'scrapy.extensions.logstats.LogStats']\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Enabled downloader middlewares:\r\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\r\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Enabled spider middlewares:\r\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Enabled item pipelines:\r\n",
      "[]\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Spider opened\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n",
      "2017-02-19 17:11:24 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n",
      "2017-02-19 17:11:24 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/robots.txt>: 'float' object is not iterable\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\r\n",
      "    result = result.throwExceptionIntoGenerator(g)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\r\n",
      "    return g.throw(self.type, self.value, self.tb)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\r\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\r\n",
      "    result = f(*args, **kw)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\r\n",
      "    return handler.download_request(request, spider)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\r\n",
      "    return agent.download_request(request)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\r\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\r\n",
      "    parsedURI.originForm)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\r\n",
      "    d = self._pool.getConnection(key, endpoint)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\r\n",
      "    return self._newConnection(key, endpoint)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\r\n",
      "    return endpoint.connect(factory)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\r\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\r\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\r\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\r\n",
      "    timeoutDelay = sum(timeout)\r\n",
      "TypeError: 'float' object is not iterable\r\n",
      "2017-02-19 17:11:24 [scrapy] ERROR: Error downloading <GET http://www.simon.rochester.edu/faculty-and-research/faculty-directory/index.aspx>\r\n",
      "TypeError: 'float' object is not iterable\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\r\n",
      "    result = result.throwExceptionIntoGenerator(g)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\r\n",
      "    return g.throw(self.type, self.value, self.tb)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\r\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\r\n",
      "    result = f(*args, **kw)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\r\n",
      "    return handler.download_request(request, spider)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 60, in download_request\r\n",
      "    return agent.download_request(request)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\", line 285, in download_request\r\n",
      "    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1631, in request\r\n",
      "    parsedURI.originForm)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\r\n",
      "    d = self._pool.getConnection(key, endpoint)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1294, in getConnection\r\n",
      "    return self._newConnection(key, endpoint)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/web/client.py\", line 1306, in _newConnection\r\n",
      "    return endpoint.connect(factory)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/endpoints.py\", line 788, in connect\r\n",
      "    EndpointReceiver, self._hostText, portNumber=self._port\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\r\n",
      "    onAddress = self._simpleResolver.getHostByName(hostName)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/scrapy/resolver.py\", line 21, in getHostByName\r\n",
      "    d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\r\n",
      "  File \"/Users/apple/anaconda/lib/python3.6/site-packages/twisted/internet/base.py\", line 276, in getHostByName\r\n",
      "    timeoutDelay = sum(timeout)\r\n",
      "TypeError: 'float' object is not iterable\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Closing spider (finished)\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Dumping Scrapy stats:\r\n",
      "{'downloader/exception_count': 2,\r\n",
      " 'downloader/exception_type_count/builtins.TypeError': 2,\r\n",
      " 'downloader/request_bytes': 501,\r\n",
      " 'downloader/request_count': 2,\r\n",
      " 'downloader/request_method_count/GET': 2,\r\n",
      " 'finish_reason': 'finished',\r\n",
      " 'finish_time': datetime.datetime(2017, 2, 19, 22, 11, 24, 516046),\r\n",
      " 'log_count/DEBUG': 1,\r\n",
      " 'log_count/ERROR': 2,\r\n",
      " 'log_count/INFO': 7,\r\n",
      " 'scheduler/dequeued': 1,\r\n",
      " 'scheduler/dequeued/memory': 1,\r\n",
      " 'scheduler/enqueued': 1,\r\n",
      " 'scheduler/enqueued/memory': 1,\r\n",
      " 'start_time': datetime.datetime(2017, 2, 19, 22, 11, 24, 95023)}\r\n",
      "2017-02-19 17:11:24 [scrapy] INFO: Spider closed (finished)\r\n"
     ]
    }
   ],
   "source": [
    "!cat faulty.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced_spider.py __init__.py        example.py         firstspider.py\r\n",
      "Basic_spider.py    \u001b[34m__pycache__\u001b[m\u001b[m        faulty.csv         scrapyorg.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# now we have the output instored in spider directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. other interesting or useful features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) logging\n",
    "Scrapy uses Python’s builtin logging system for event logging. Logging works out of the box, and can be configured to some extent with the Scrapy settings listed in Logging settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This is a warning\n"
     ]
    }
   ],
   "source": [
    "# a simply logging message using the logging.WARNING level:\n",
    "import logging\n",
    "logging.warning(\"This is a warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can put logging inside a spider\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://scrapinghub.com']\n",
    "    def parse(self, response):\n",
    "        self.logger.info('Parse function called on %s', response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) sending email\n",
    "Scrapy provides its own facility for sending e-mails which is very easy to use and it’s implemented using Twisted non-blocking IO, to avoid interfering with the non-blocking IO of the crawler. It also provides a simple API for sending attachments and it’s very easy to configure, with a few settings.\n",
    "\n",
    "#### syntax\n",
    "class scrapy.mail.MailSender(smtphost=None, mailfrom=None, smtpuser=None, smtppass=None,\n",
    "smtpport=None)\n",
    "send(to, subject, body, cc=None, attachs=(), mimetype=’text/plain’, charset=None)\n",
    "\n",
    "- Parameters\n",
    "    - to – the e-mail recipients\n",
    "    - subject – the subject of the e-mail\n",
    "    - cc – the e-mails to CC\n",
    "    - body – the e-mail body\n",
    "    - attachs(attach_name, mimetype, file_object) – an iterable of tuples  where attach_name is a string with the name that will appear on the e-mail’s attachment, mimetype is the mimetype of the attachment and file_object is a readable file object with the contents of the attachment\n",
    "    - mimetype  – the MIME type of the e-mail\n",
    "    - charset – the character encoding to use for the e-mail contents\n",
    "\n",
    "\n",
    "- The following command define the MailSender class, and can be used to configure setting:\n",
    "    - subject – the subject of the e-mail\n",
    "    - cc – the e-mails to CC\n",
    "    - MAIL_FROM\n",
    "    Default: ’scrapy@localhost’\n",
    "    Sender email to use (From: header) for sending emails.\n",
    "    - MAIL_HOST\n",
    "    Default: ’localhost’\n",
    "    SMTP host to use for sending emails.\n",
    "    - MAIL_PORT\n",
    "    Default: 25\n",
    "    MTP port to use for sending emails.\n",
    "    - MAIL_USER\n",
    "    Default: None\n",
    "    User to use for SMTP authentication. If disabled no SMTP authentication will be performed.\n",
    "    - MAIL_PASS\n",
    "    Default: None\n",
    "    Password to use for SMTP authentication, along with MAIL_USER.\n",
    "\n",
    "- Note: Scrapy does not support sending mail with Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scrapy.mail import MailSender\n",
    "mailer = MailSender()\n",
    "mailer.send(to=[\"Pinyi.Liu@simon.rochester.edu\"], subject=\"scrapy tool\", body=\"Hi! This is scrapy\", \n",
    "            cc=[\"rippleslpy@gamil.com\"])\n",
    "# Note: Scrapy does not support sending mail with Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Core API\n",
    "\n",
    "- Use those to measure prediction accuracy \n",
    "   \n",
    "    \n",
    " - Scrapy core API is used for developers of extensions and middlewares.\n",
    " The following are some typical types of API:\n",
    "    - Crawler API:\n",
    "      - Crawler object is the main entry point to Scrapy API. It is passed to extensions through the from_crawler class method. This object provides access to all Scrapy core components, and it’s the only way for extensions to access them and hook their functionality into Scrapy. \n",
    " \n",
    "    - Settings API\n",
    "       - It sets the key name and priority level of the default settings priorities used in Scrapy. Each item defines a settings entry point, giving it a code name for identification and an integer priority. Greater priorities take more precedence over lesser ones when setting and retrieving values in the Settings class.\n",
    "   \n",
    "   - SpiderLoader API\n",
    "     - It is in charge of retrieving and handling the spider classes defined across the project. Custom spider loaders can be employed by specifying their path in the SPIDER_LOADER_CLASS project setting. They must fully implement the scrapy.interfaces.ISpiderLoader interface to guarantee an errorless execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Signals\n",
    "Scrapy uses signals extensively to notify when certain events occur. You can catch some of those signals in your Scrapy project to perform additional tasks or extend Scrapy to add functionality not provided out of the box. You can connect to signals (or send your own) through the Signals API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
